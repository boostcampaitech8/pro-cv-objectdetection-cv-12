W&B logging initialized. To monitor logs, open https://wandb.ai/kimbum/RFDETR/runs/ch0kebms.
Not using distributed mode
fatal: not a git repository (or any parent up to mount point /data)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
git:
  sha: N/A, status: clean, branch: N/A

Namespace(num_classes=10, grad_accum_steps=4, amp=True, lr=0.0001, lr_encoder=0.00015, batch_size=4, weight_decay=0.0001, epochs=32, lr_drop=100, clip_max_norm=0.1, lr_vit_layer_decay=0.8, lr_component_decay=0.7, do_benchmark=False, dropout=0, drop_path=0.0, drop_mode='standard', drop_schedule='constant', cutoff_epoch=0, pretrained_encoder=None, pretrain_weights='rf-detr-medium.pth', pretrain_exclude_keys=None, pretrain_keys_modify_to_load=None, pretrained_distiller=None, encoder='dinov2_windowed_small', vit_encoder_num_layers=12, window_block_indexes=None, position_embedding='sine', out_feature_indexes=[3, 6, 9, 12], freeze_encoder=False, layer_norm=True, rms_norm=False, backbone_lora=False, force_no_pretrain=False, dec_layers=4, dim_feedforward=2048, hidden_dim=256, sa_nheads=8, ca_nheads=16, num_queries=300, group_detr=13, two_stage=True, projector_scale=['P4'], lite_refpoint_refine=True, num_select=300, dec_n_points=2, decoder_norm='LN', bbox_reparam=True, freeze_batch_norm=False, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=1.0, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, aux_loss=True, sum_group_losses=False, use_varifocal_loss=False, use_position_supervised_loss=False, ia_bce_loss=True, dataset_file='roboflow', coco_path=None, dataset_dir='../../../dataset_split15/', square_resize_div_64=True, output_dir='./RF_DETR_ckpt', dont_save_weights=False, checkpoint_interval=5, seed=42, resume='', start_epoch=0, eval=False, use_ema=True, ema_decay=0.993, ema_tau=100, num_workers=2, device='cuda', world_size=1, dist_url='env://', sync_bn=True, fp16_eval=False, encoder_only=False, backbone_only=False, resolution=1024, use_cls_token=False, multi_scale=True, expanded_scales=True, do_random_resize_via_padding=True, warmup_epochs=0.0, lr_scheduler='step', lr_min_factor=0.0, early_stopping=True, early_stopping_patience=5, early_stopping_min_delta=0.001, early_stopping_use_ema=False, gradient_checkpointing=False, patch_size=16, num_windows=2, positional_encoding_size=36, mask_downsample_ratio=4, tensorboard=True, wandb=True, project='RFDETR', run='RFDETR_medium', class_names=['General trash', 'Paper', 'Paper pack', 'Metal', 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery', 'Clothing'], run_test=True, segmentation_head=False, distributed=False)
number of params: 33396020
[864, 896, 928, 960, 992, 1024, 1056, 1088, 1120, 1152, 1184]
loading annotations into memory...
Done (t=0.07s)
creating index...
index created!
[864, 896, 928, 960, 992, 1024, 1056, 1088, 1120, 1152, 1184]
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
[864, 896, 928, 960, 992, 1024, 1056, 1088, 1120, 1152, 1184]
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
Get benchmark
Start training
Grad accum steps:  4
Total batch size:  16
LENGTH OF DATA LOADER: 260
UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)
UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)
Epoch: [0]  [  0/260]  eta: 0:38:19  lr: 0.000100  class_error: 78.63  loss: 13.5064 (13.5064)  loss_ce: 1.0334 (1.0334)  loss_bbox: 0.7061 (0.7061)  loss_giou: 0.8389 (0.8389)  loss_ce_0: 0.9511 (0.9511)  loss_bbox_0: 0.8555 (0.8555)  loss_giou_0: 0.9628 (0.9628)  loss_ce_1: 1.0311 (1.0311)  loss_bbox_1: 0.6438 (0.6438)  loss_giou_1: 0.8650 (0.8650)  loss_ce_2: 1.0310 (1.0310)  loss_bbox_2: 0.5758 (0.5758)  loss_giou_2: 0.7545 (0.7545)  loss_ce_enc: 0.8705 (0.8705)  loss_bbox_enc: 1.2007 (1.2007)  loss_giou_enc: 1.1863 (1.1863)  loss_ce_unscaled: 1.0334 (1.0334)  class_error_unscaled: 78.6325 (78.6325)  loss_bbox_unscaled: 0.1412 (0.1412)  loss_giou_unscaled: 0.4195 (0.4195)  cardinality_error_unscaled: 3839.0000 (3839.0000)  loss_ce_0_unscaled: 0.9511 (0.9511)  loss_bbox_0_unscaled: 0.1711 (0.1711)  loss_giou_0_unscaled: 0.4814 (0.4814)  cardinality_error_0_unscaled: 3795.7500 (3795.7500)  loss_ce_1_unscaled: 1.0311 (1.0311)  loss_bbox_1_unscaled: 0.1288 (0.1288)  loss_giou_1_unscaled: 0.4325 (0.4325)  cardinality_error_1_unscaled: 3847.2500 (3847.2500)  loss_ce_2_unscaled: 1.0310 (1.0310)  loss_bbox_2_unscaled: 0.1152 (0.1152)  loss_giou_2_unscaled: 0.3773 (0.3773)  cardinality_error_2_unscaled: 3839.7500 (3839.7500)  loss_ce_enc_unscaled: 0.8705 (0.8705)  loss_bbox_enc_unscaled: 0.2401 (0.2401)  loss_giou_enc_unscaled: 0.5931 (0.5931)  cardinality_error_enc_unscaled: 3874.5000 (3874.5000)  time: 8.8442  data: 1.6097  max mem: 25758
Using a different number of positional encodings than DINOv2, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.
Using patch size 16 instead of 14, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.
Loading pretrain weights
TensorBoard logging initialized. To monitor logs, use 'tensorboard --logdir ./RF_DETR_ckpt' and open http://localhost:6006/ in browser.
