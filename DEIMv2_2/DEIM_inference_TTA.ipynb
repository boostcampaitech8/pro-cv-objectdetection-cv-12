{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f9bbd0c-dc87-466e-bbf3-54e1c6bd931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from engine.core import YAMLConfig\n",
    "from engine.misc import dist_utils\n",
    "from engine.data.transforms import Compose, Normalize, Resize, ConvertPILImage\n",
    "from engine.solver import TASKS\n",
    "\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "from ensemble_boxes import weighted_boxes_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25a366-7fda-4b28-adb1-a8cd00c90c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfine_solver(config_path, checkpoint_path, device=\"cuda\"):\n",
    "\n",
    "    update_dict = {\n",
    "        \"resume\": None,\n",
    "        \"device\": device,\n",
    "        \"seed\": 42,\n",
    "        \"tuning\": checkpoint_path,\n",
    "        \"use_amp\": False,\n",
    "        \"use_ema\": True,\n",
    "    }\n",
    "\n",
    "    cfg = YAMLConfig(config_path, **update_dict)\n",
    "\n",
    "    if \"HGNetv2\" in cfg.yaml_cfg:\n",
    "        cfg.yaml_cfg[\"HGNetv2\"][\"pretrained\"] = False\n",
    "\n",
    "    # Solver ÏÉùÏÑ±\n",
    "    SolverClass = TASKS[cfg.yaml_cfg[\"task\"]]\n",
    "    solver = SolverClass(cfg)\n",
    "\n",
    "    solver._setup()\n",
    "\n",
    "    # checkpoint load Î∞©ÏãùÎèÑ train.pyÏôÄ ÎèôÏùº\n",
    "    ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    state = ckpt[\"model\"] if \"model\" in ckpt else ckpt[\"ema\"][\"module\"]\n",
    "    solver.model.load_state_dict(state, strict=False)\n",
    "\n",
    "    solver.model.to(device)\n",
    "    solver.model.eval()\n",
    "    return solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55036ec1-e513-415d-8573-17da7d4ca005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfine_test_inference_tta(\n",
    "    config,\n",
    "    checkpoint,\n",
    "    image_dir,\n",
    "    output_csv=\"output.csv\",\n",
    "    threshold=0.01,\n",
    "    base_size=1024,           # Î™®Îç∏ ÏûÖÎ†• Í∏∞Ï§Ä Ìï¥ÏÉÅÎèÑ\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    print(f\"Loading model with Flip TTA enabled (Normalized)...\")\n",
    "    solver = load_dfine_solver(config, checkpoint, device)\n",
    "    model = solver.model\n",
    "    postprocessor = solver.postprocessor\n",
    "    model.eval()\n",
    "\n",
    "    image_paths = sorted(list(Path(image_dir).glob(\"*.jpg\"))) + \\\n",
    "                  sorted(list(Path(image_dir).glob(\"*.png\")))\n",
    "\n",
    "    predictions = []\n",
    "    filenames = []\n",
    "\n",
    "    print(f\"Starting Inference with Flip TTA (Original + Horizontal Flip)...\")\n",
    "    \n",
    "    #Ï†ïÍ∑úÌôî(Normalize)\n",
    "    tfs = Compose([\n",
    "        Resize((base_size, base_size)),\n",
    "        lambda x: to_tensor(x).type(torch.float32),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "    ])\n",
    "\n",
    "    for img_path in tqdm(image_paths):\n",
    "        # Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "        img_pil = Image.open(img_path).convert(\"RGB\")\n",
    "        w0, h0 = img_pil.size\n",
    "        \n",
    "        # Ï†ÑÏ≤òÎ¶¨ (Resize -> Tensor -> Normalize)\n",
    "        img_tensor = tfs(img_pil) # [C, H, W]\n",
    "        \n",
    "        box_list = []\n",
    "        score_list = []\n",
    "        label_list = []\n",
    "\n",
    "        # TTA Ï†ÑÎûµ\n",
    "        img_variants = [\n",
    "            (img_tensor.unsqueeze(0).to(device), False),          # Original\n",
    "            (torch.flip(img_tensor, [2]).unsqueeze(0).to(device), True) # H-Flip\n",
    "        ]\n",
    "\n",
    "        # Ï∂îÎ°†\n",
    "        for img_input, is_flipped in img_variants:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(img_input)\n",
    "            \n",
    "            # Post-process \n",
    "            pred = postprocessor(\n",
    "                outputs,\n",
    "                orig_target_sizes=torch.tensor([[h0, w0]], device=device)\n",
    "            )[0]\n",
    "\n",
    "            boxes = pred[\"boxes\"].cpu().numpy()  # [x1, y1, x2, y2]\n",
    "            scores = pred[\"scores\"].cpu().numpy()\n",
    "            labels = pred[\"labels\"].cpu().numpy()\n",
    "\n",
    "            # Ï¢åÏö∞ Î∞òÏ†Ñ Î≥µÍµ¨\n",
    "            if is_flipped: \n",
    "                original_x1 = boxes[:, 0].copy()\n",
    "                original_x2 = boxes[:, 2].copy()\n",
    "                boxes[:, 0] = w0 - original_x2\n",
    "                boxes[:, 2] = w0 - original_x1\n",
    "\n",
    "            # WBFÎ•º ÏúÑÌïú Ï†ïÍ∑úÌôî (0~1)\n",
    "            boxes[:, [0, 2]] = boxes[:, [0, 2]] / w0\n",
    "            boxes[:, [1, 3]] = boxes[:, [1, 3]] / h0\n",
    "            boxes = np.clip(boxes, 0, 1)\n",
    "\n",
    "            box_list.append(boxes.tolist())\n",
    "            score_list.append(scores.tolist())\n",
    "            label_list.append(labels.tolist())\n",
    "\n",
    "        # [ÏïôÏÉÅÎ∏î] Weighted Box Fusion (WBF)\n",
    "        if len(box_list) > 0:\n",
    "            final_boxes, final_scores, final_labels = weighted_boxes_fusion(\n",
    "                box_list, \n",
    "                score_list, \n",
    "                label_list, \n",
    "                weights=[1, 1],    # ÏõêÎ≥∏Í≥º Flip ÎèôÏùº Í∞ÄÏ§ëÏπò\n",
    "                iou_thr=0.6,       # Í≤πÏπ® ÏûÑÍ≥ÑÍ∞í\n",
    "                skip_box_thr=0.1\n",
    "            )\n",
    "            \n",
    "            # Ï†ïÍ∑úÌôîÎêú Ï¢åÌëúÎ•º Îã§Ïãú ÌîΩÏÖÄ Ï¢åÌëúÎ°ú Î≥µÍµ¨\n",
    "            final_boxes[:, [0, 2]] *= w0\n",
    "            final_boxes[:, [1, 3]] *= h0\n",
    "            \n",
    "        else:\n",
    "            final_boxes, final_scores, final_labels = [], [], []\n",
    "\n",
    "        # Í≤∞Í≥º Î¨∏ÏûêÏó¥ ÏÉùÏÑ±\n",
    "        pred_str = \"\"\n",
    "        for box, score, label in zip(final_boxes, final_scores, final_labels):\n",
    "            if score < threshold:\n",
    "                continue\n",
    "            \n",
    "            label = int(label)\n",
    "            x1, y1, x2, y2 = box\n",
    "            pred_str += f\"{label} {score:.4f} {x1:.2f} {y1:.2f} {x2:.2f} {y2:.2f} \"\n",
    "\n",
    "        predictions.append(pred_str.strip())\n",
    "        filenames.append(f\"test/{img_path.name}\")\n",
    "\n",
    "    # CSV Ï†ÄÏû•\n",
    "    df = pd.DataFrame({\n",
    "        \"PredictionString\": predictions,\n",
    "        \"image_id\": filenames,\n",
    "    })\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"üìÑ Normalized Flip TTA Complete! Saved to ‚Üí {output_csv}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d65a5-efa6-47d4-bb4a-59a440e7f4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading model with Flip TTA enabled (Normalized)...\n",
      "Training DINOv3 from scratch...\n",
      "Using Lite Spatial Prior Module with inplanes=64\n",
      "     --- Use Gateway@True ---\n",
      "     --- Use Share Bbox Head@False ---\n",
      "     --- Use Share Score Head@False ---\n",
      "     --- Wide Layer@1 ---\n",
      "Tuning checkpoint from ./outputs/deimv2_dinov3_x_coco/checkpoint0019.pth\n",
      "Load model.state_dict, {'missed': [], 'unmatched': []}\n",
      "Using the new matching cost with iou_order_alpha = 4.0 at epoch 45\n",
      "üöÄ Starting Inference with Flip TTA (Original + Horizontal Flip)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4871/4871 [25:59<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Normalized Flip TTA Complete! Saved to ‚Üí deimv2_submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7 0.9810 604.24 517.68 956.28 1024.00 7 0.9763...</td>\n",
       "      <td>test/0000.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 0.2884 344.70 250.02 751.98 694.22 3 0.2363 ...</td>\n",
       "      <td>test/0001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 0.9492 775.87 406.84 1023.78 1023.88 4 0.748...</td>\n",
       "      <td>test/0002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9 0.6638 145.99 262.45 911.19 823.58 9 0.0364 ...</td>\n",
       "      <td>test/0003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0 0.3310 425.33 408.11 656.75 577.95 1 0.2268 ...</td>\n",
       "      <td>test/0004.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    PredictionString       image_id\n",
       "0  7 0.9810 604.24 517.68 956.28 1024.00 7 0.9763...  test/0000.jpg\n",
       "1  4 0.2884 344.70 250.02 751.98 694.22 3 0.2363 ...  test/0001.jpg\n",
       "2  1 0.9492 775.87 406.84 1023.78 1023.88 4 0.748...  test/0002.jpg\n",
       "3  9 0.6638 145.99 262.45 911.19 823.58 9 0.0364 ...  test/0003.jpg\n",
       "4  0 0.3310 425.33 408.11 656.75 577.95 1 0.2268 ...  test/0004.jpg"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfine_test_inference_tta(\n",
    "    config=\"./configs/deimv2/deimv2_dinov3_x_coco.yml\",\n",
    "    checkpoint=\"./outputs/deimv2_dinov3_x_coco/checkpoint0019.pth\",\n",
    "    image_dir=\"../dataset/test\",\n",
    "    output_csv=\"deimv2_submission.csv\",\n",
    "    threshold=0.01\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyth\non (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
