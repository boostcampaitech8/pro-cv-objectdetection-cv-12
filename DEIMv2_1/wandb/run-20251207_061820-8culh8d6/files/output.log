
------------------------------------- Calculate Flops Results -------------------------------------
Notations:
number of parameters (Params), number of multiply-accumulate operations(MACs),
number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),
fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),
default model backpropagation takes 2.00 times as much computation as forward propagation.

Total Training Params:                                                  50.21 M
fwd MACs:                                                               188.641 GMACs
fwd FLOPs:                                                              377.827 GFLOPS
fwd+bwd MACs:                                                           565.924 GMACs
fwd+bwd FLOPs:                                                          1.1335 TFLOPS
---------------------------------------------------------------------------------------------------
{'Model FLOPs:377.827 GFLOPS   MACs:188.641 GMACs   Params:50206263'}
------------------------------------------Start training-------------------------------------------
Index 194: backbone.dinov3.blocks.11.mlp.w3.weight - requires_grad: True
Index 195: backbone.dinov3.blocks.11.mlp.w3.bias - requires_grad: True
     ## Using Self-defined Scheduler-flatcosine ##
[1e-05, 1e-05, 0.0005, 0.0005] [5e-06, 5e-06, 0.00025, 0.00025] 31488 2000 13776 11808
number of trainable parameters: 51067884
number of non-trainable parameters: 2
/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/solver/det_engine.py:104: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  if not math.isfinite(loss_value):
Epoch: [0]  [  0/984]  eta: 1:05:44  lr: 0.000000  loss: 72.8784 (72.8784)  loss_mal: 1.2529 (1.2529)  loss_bbox: 1.3615 (1.3615)  loss_giou: 1.9216 (1.9216)  loss_fgl: 0.3665 (0.3665)  loss_mal_aux_0: 0.9966 (0.9966)  loss_bbox_aux_0: 1.5313 (1.5313)  loss_giou_aux_0: 2.1015 (2.1015)  loss_fgl_aux_0: 0.4139 (0.4139)  loss_ddf_aux_0: 0.1505 (0.1505)  loss_mal_aux_1: 1.0615 (1.0615)  loss_bbox_aux_1: 1.4085 (1.4085)  loss_giou_aux_1: 1.9731 (1.9731)  loss_fgl_aux_1: 0.3871 (0.3871)  loss_ddf_aux_1: 0.0405 (0.0405)  loss_mal_aux_2: 1.1895 (1.1895)  loss_bbox_aux_2: 1.3666 (1.3666)  loss_giou_aux_2: 1.9679 (1.9679)  loss_fgl_aux_2: 0.3544 (0.3544)  loss_ddf_aux_2: 0.0067 (0.0067)  loss_mal_aux_3: 1.3428 (1.3428)  loss_bbox_aux_3: 1.3588 (1.3588)  loss_giou_aux_3: 1.9226 (1.9226)  loss_fgl_aux_3: 0.3664 (0.3664)  loss_ddf_aux_3: 0.0004 (0.0004)  loss_mal_aux_4: 1.3936 (1.3936)  loss_bbox_aux_4: 1.3619 (1.3619)  loss_giou_aux_4: 1.9219 (1.9219)  loss_fgl_aux_4: 0.3664 (0.3664)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_mal_pre: 0.9751 (0.9751)  loss_bbox_pre: 1.5436 (1.5436)  loss_giou_pre: 2.0968 (2.0968)  loss_mal_enc_0: 2.8828 (2.8828)  loss_bbox_enc_0: 1.9207 (1.9207)  loss_giou_enc_0: 2.5821 (2.5821)  loss_mal_dn_0: 1.1689 (1.1689)  loss_bbox_dn_0: 0.9004 (0.9004)  loss_giou_dn_0: 1.4049 (1.4049)  loss_fgl_dn_0: 0.8682 (0.8682)  loss_ddf_dn_0: 0.1893 (0.1893)  loss_mal_dn_1: 1.5020 (1.5020)  loss_bbox_dn_1: 0.7867 (0.7867)  loss_giou_dn_1: 1.2032 (1.2032)  loss_fgl_dn_1: 0.8686 (0.8686)  loss_ddf_dn_1: 0.0727 (0.0727)  loss_mal_dn_2: 1.7764 (1.7764)  loss_bbox_dn_2: 0.7201 (0.7201)  loss_giou_dn_2: 1.1031 (1.1031)  loss_fgl_dn_2: 0.8796 (0.8796)  loss_ddf_dn_2: 0.0252 (0.0252)  loss_mal_dn_3: 1.9756 (1.9756)  loss_bbox_dn_3: 0.7375 (0.7375)  loss_giou_dn_3: 1.1017 (1.1017)  loss_fgl_dn_3: 0.8747 (0.8747)  loss_ddf_dn_3: 0.0059 (0.0059)  loss_mal_dn_4: 2.1328 (2.1328)  loss_bbox_dn_4: 0.7071 (0.7071)  loss_giou_dn_4: 1.0742 (1.0742)  loss_fgl_dn_4: 0.8896 (0.8896)  loss_ddf_dn_4: -0.0000 (-0.0000)  loss_mal_dn_5: 1.8887 (1.8887)  loss_bbox_dn_5: 0.7074 (0.7074)  loss_giou_dn_5: 1.0741 (1.0741)  loss_fgl_dn_5: 0.8893 (0.8893)  loss_mal_dn_pre: 1.1963 (1.1963)  loss_bbox_dn_pre: 0.8862 (0.8862)  loss_giou_dn_pre: 1.3803 (1.3803)  time: 4.0088  data: 0.5103  max mem: 11574
Epoch: [0]  [500/984]  eta: 0:06:48  lr: 0.000001  loss: 71.9101 (80.5139)  loss_mal: 2.8008 (2.2553)  loss_bbox: 0.4741 (1.3186)  loss_giou: 0.4666 (1.1249)  loss_fgl: 0.7312 (0.7140)  loss_mal_aux_0: 2.6113 (1.7901)  loss_bbox_aux_0: 0.7230 (1.5549)  loss_giou_aux_0: 0.7500 (1.3390)  loss_fgl_aux_0: 1.0364 (0.8047)  loss_ddf_aux_0: 0.5320 (0.2496)  loss_mal_aux_1: 2.3867 (1.9157)  loss_bbox_aux_1: 0.4840 (1.3901)  loss_giou_aux_1: 0.5408 (1.1947)  loss_fgl_aux_1: 0.8341 (0.7332)  loss_ddf_aux_1: 0.1190 (0.0645)  loss_mal_aux_2: 2.7344 (2.1795)  loss_bbox_aux_2: 0.4146 (1.3460)  loss_giou_aux_2: 0.4872 (1.1529)  loss_fgl_aux_2: 0.7329 (0.7024)  loss_ddf_aux_2: 0.0291 (0.0146)  loss_mal_aux_3: 3.1992 (2.4521)  loss_bbox_aux_3: 0.4333 (1.3246)  loss_giou_aux_3: 0.4968 (1.1317)  loss_fgl_aux_3: 0.7263 (0.7096)  loss_ddf_aux_3: 0.0043 (0.0016)  loss_mal_aux_4: 3.2207 (2.5408)  loss_bbox_aux_4: 0.4740 (1.3188)  loss_giou_aux_4: 0.4677 (1.1250)  loss_fgl_aux_4: 0.7312 (0.7138)  loss_ddf_aux_4: 0.0001 (0.0000)  loss_mal_pre: 2.5820 (1.7543)  loss_bbox_pre: 0.7273 (1.5922)  loss_giou_pre: 0.7469 (1.3721)  loss_mal_enc_0: 1.9375 (3.5495)  loss_bbox_enc_0: 1.2753 (2.0881)  loss_giou_enc_0: 1.2169 (1.7810)  loss_mal_dn_0: 1.8740 (1.5851)  loss_bbox_dn_0: 1.0530 (1.3338)  loss_giou_dn_0: 0.9720 (1.1538)  loss_fgl_dn_0: 1.0415 (1.0233)  loss_ddf_dn_0: 0.5267 (0.2856)  loss_mal_dn_1: 2.2070 (1.9310)  loss_bbox_dn_1: 0.6987 (1.0753)  loss_giou_dn_1: 0.6696 (0.9301)  loss_fgl_dn_1: 0.9667 (0.9812)  loss_ddf_dn_1: 0.1706 (0.0923)  loss_mal_dn_2: 2.6602 (2.2406)  loss_bbox_dn_2: 0.6483 (0.9876)  loss_giou_dn_2: 0.5511 (0.8571)  loss_fgl_dn_2: 0.8885 (0.9492)  loss_ddf_dn_2: 0.0420 (0.0276)  loss_mal_dn_3: 2.9551 (2.5443)  loss_bbox_dn_3: 0.6150 (0.9475)  loss_giou_dn_3: 0.5307 (0.8249)  loss_fgl_dn_3: 0.8899 (0.9615)  loss_ddf_dn_3: 0.0066 (0.0050)  loss_mal_dn_4: 3.0742 (2.6816)  loss_bbox_dn_4: 0.5810 (0.9271)  loss_giou_dn_4: 0.5128 (0.8046)  loss_fgl_dn_4: 0.8989 (0.9734)  loss_ddf_dn_4: 0.0000 (0.0000)  loss_mal_dn_5: 2.5996 (2.3714)  loss_bbox_dn_5: 0.5812 (0.9271)  loss_giou_dn_5: 0.5126 (0.8045)  loss_fgl_dn_5: 0.8988 (0.9736)  loss_mal_dn_pre: 1.8574 (1.6317)  loss_bbox_dn_pre: 1.0574 (1.2610)  loss_giou_dn_pre: 0.9716 (1.1211)  time: 0.8213  data: 0.0114  max mem: 15066
Traceback (most recent call last):
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/train.py", line 87, in <module>
    main(args)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/train.py", line 57, in main
    solver.fit()
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/solver/det_solver.py", line 91, in fit
    train_stats = train_one_epoch(
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/solver/det_engine.py", line 50, in train_one_epoch
    outputs = model(samples, targets=targets)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/deim/deim.py", line 29, in forward
    x = self.decoder(x, targets)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/deim/deim_decoder.py", line 547, in forward
    out_bboxes, out_logits, out_corners, out_refs, pre_bboxes, pre_logits = self.decoder(
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/deim/deim_decoder.py", line 189, in forward
    output = layer(output, ref_points_input, value, spatial_shapes, attn_mask, query_pos_embed)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/deim/deim_decoder.py", line 90, in forward
    target2 = self.cross_attn(\
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/deim/dfine_decoder.py", line 144, in forward
    output = self.ms_deformable_attn_core(value, value_spatial_shapes, sampling_locations, attention_weights, self.num_points_list)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/deim/utils.py", line 139, in deformable_attention_core_func_v2
    attn_weights = attention_weights.permute(0, 2, 1, 3).reshape(bs * n_head, 1, Len_q, sum(num_points_list))
KeyboardInterrupt
Traceback (most recent call last):
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/train.py", line 87, in <module>
    main(args)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/train.py", line 57, in main
    solver.fit()
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/solver/det_solver.py", line 91, in fit
    train_stats = train_one_epoch(
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/solver/det_engine.py", line 50, in train_one_epoch
    outputs = model(samples, targets=targets)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/deim/deim.py", line 29, in forward
    x = self.decoder(x, targets)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/deim/deim_decoder.py", line 547, in forward
    out_bboxes, out_logits, out_corners, out_refs, pre_bboxes, pre_logits = self.decoder(
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/deim/deim_decoder.py", line 189, in forward
    output = layer(output, ref_points_input, value, spatial_shapes, attn_mask, query_pos_embed)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/deim/deim_decoder.py", line 90, in forward
    target2 = self.cross_attn(\
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/ephemeral/home/pybum/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/deim/dfine_decoder.py", line 144, in forward
    output = self.ms_deformable_attn_core(value, value_spatial_shapes, sampling_locations, attention_weights, self.num_points_list)
  File "/data/ephemeral/home/pybum_bumjin/DEIMv2/engine/deim/utils.py", line 139, in deformable_attention_core_func_v2
    attn_weights = attention_weights.permute(0, 2, 1, 3).reshape(bs * n_head, 1, Len_q, sum(num_points_list))
KeyboardInterrupt
